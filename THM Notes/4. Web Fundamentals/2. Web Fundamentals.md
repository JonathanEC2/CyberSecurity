# Introduction to Web Hacking
## Exploring the Website

As a pen tester, your role when reviewing a website or web application is to discover features that could potentially be vulnerable and attempt to exploit them to assess whether or not they are. These features generally involve some interactivity with the user. 

Explore the website and note down the individual pages/areas/features with a summary for each one.

## Viewing the Page Source

The page source is the human-readable code returned to our browser/client from the web server each time we make a request.

- Always check for comments
- Explore all directories
- Check for clues of what web framework is being used. Knowing the framework and version can be a powerful find as there may be public vulnerabilities in the framework

## Developer Tools - Inspector

The page source doesn't always represent what's shown on a webpage. ; this is because CSS, JavaScript and user interaction can change the content and style of the page, which means we need a way to view what's been displayed in the browser window at this exact time. Element inspector assists us with this by providing us with a live representation of what is currently on the website.

## Developer Tools - Debugger

Called sources in Google Chrome

 Breakpoints are points in the code that we can force the browser to stop processing the JavaScript and pause the current execution. If you click the line number that contains the code, it creates a breakpoint.

## Developer Tools - Network

The network tab on the developer tools can be used to keep track of every external request a webpage makes. If you click on the Network tab and then refresh the page, you'll see all the files the page is requesting. 

AJAX is a method for sending and receiving network data in a web application background without interfering by changing the current web page.

# Content Discovery

When we talk about discovery, we are talking about the things that are not immediately presented to us and that weren't intended for us to have access to. This content could be, for example, pages or portals intended for staff usage, older versions of the website, backup files, configuration files, administration panels, etc.

There are three main ways of discovering content on a website:
- Manually
- Automated
- OSINT (Open-Source Intelligence)

## Manual Discover

#### Robots.txt

The robots.txt file is a document that tells search engines which pages they are and aren't allowed to show on their search engine results or ban specific search engines from crawling the website altogether. This file gives us a great list of locations on the website that the owners don't want us to discover as penetration testers.

#### Favicon

The favicon is a small icon displayed in the browser's address bar or tab used for branding a website. Sometimes when frameworks are used to build a website, a favicon that is part of the installation gets leftover, and if the website developer doesn't replace this with a custom one, this can give us a clue on what framework is in use.

#### Sitemap.xml

sitemap.xml file gives a list of every file the website owner wishes to be listed on a search engine. These can sometimes contain areas of the website that are a bit more difficult to navigate to or even list some old webpages that the current site no longer uses but are still working behind the scenes.

#### HTTP Headers

When we make requests to the web server, the server returns various HTTP headers. These headers can sometimes contain useful information such as the webserver software and possibly the programming/scripting language in use. Using this information, we could find vulnerable versions of software being used.

```
curl WEBPAGE_DOMAIN -v
```

#### Framework Stack

Once you've established the framework of a website, you can then locate the framework's website. From there, we can learn more about the software and other information, possibly leading to more content we can discover.

## OSINT

#### Google Hacking/Dorking

| **Filter** | **Example**        | **Description**                                              |
| ---------- | ------------------ | ------------------------------------------------------------ |
| site       | site:tryhackme.com | returns results only from the specified website address      |
| inurl      | inurl:admin        | returns results that have the specified word in the URL      |
| filetype   | filetype:pdf       | returns results which are a particular file extension        |
| intitle    | intitle:admin      | returns results that contain the specified word in the title |
#### Wappalyzer

Wappalyzer (https://www.wappalyzer.com/) is an online tool and browser extension that helps identify what technologies a website uses, such as frameworks, Content Management Systems (CMS), payment processors and much more

#### Wayback Machine

The Wayback Machine (https://archive.org/web/) is a historical archive of websites that dates back to the late 90s. You can search a domain name, and it will show you all the times the service scraped the web page and saved the contents. This service can help uncover old pages that may still be active on the current website.

#### GitHub

 Git is a version control system that tracks changes to files in a project. Working in a team is easier because you can see what each team member is editing and what changes they made to files. When users have finished making their changes, they commit them with a message and then push them back to a central location (repository) for the other users to then pull those changes to their local machines. You can use GitHub's search feature to look for company names or website names to try and locate repositories belonging to your target. Once discovered, you may have access to source code, passwords or other content that you hadn't yet found.

#### S3 Buckets

S3 Buckets are a storage service provided by Amazon AWS, allowing people to save files and even static website content in the cloud accessible over HTTP and HTTPS. Sometimes S3 bucket permissions are incorrectly set and inadvertently allow access to files that shouldn't be available to the public. The format of the S3 buckets is http(s)://{name}.s3.amazonaws.com where {name} is decided by the owner

## Automate Discovery 

```
ffuf -w /usr/share/wordlists/SecLists/Discovery/Web-Content/common.txt -u WEBSITE/FUZZ
```

```
dirb WEBSITE /usr/share/wordlists/SecLists/Discovery/Web-Content/common.txt
```

```
gobuster dir --url WEEBSITE -w /usr/share/wordlists/SecLists/Discovery/Web-Content/common.txt
```

Wordlist: https://github.com/danielmiessler/SecLists

# Subdomain Enumeration

Subdomain enumeration is the process of finding valid subdomains for a domain, but why do we do this. three different subdomain enumeration methods: 
- Brute Force
- OSINT (Open-Source Intelligence)
- Virtual Host.

## OSINT

#### SSL/TLS Certificates

When an SSL/TLS certificate is created for a domain by a CA (Certificate Authority), CA's take part in what's called "Certificate Transparency (CT) logs". These are publicly accessible logs of every SSL/TLS certificate created for a domain name. The purpose of Certificate Transparency logs is to stop malicious and accidentally made certificates from being used. We can use this service to our advantage to discover subdomains belonging to a domain, sites like https://crt.sh offer a searchable database of certificates that shows current and historical results.

#### Search Engines

Using advanced search methods on websites like Google, such as the <mark style="background: #D2B3FFA6;">site: filter</mark>, can narrow the search results. For example, <mark style="background: #D2B3FFA6;">site:*.domain.com -site:www.domain.com</mark> would only contain results leading to the domain name domain.com but exclude any links to www.domain.com; therefore, it shows us only subdomain names belonging to domain.com.

#### DNS Bruteforce

Bruteforce DNS (Domain Name System) enumeration is the method of trying tens, hundreds, thousands or even millions of different possible subdomains from a pre-defined list of commonly used subdomains. Because this method requires many requests, we automate it with tools to make the process quicker.

```
dnsrecon -t brt -d acmeitsupport.thm
```

```
./sublist3r.py -d acmeitsupport.thm
```

## Virtual Hosts
Some subdomains aren't always hosted in publically accessible DNS results, such as development versions of a web application or administration portals. Instead, the DNS record could be kept on a private DNS server or recorded on the developer's machines in their <span style="color:rgb(0, 176, 80)">/etc/hosts</span> file (or <span style="color:rgb(0, 176, 80)">c:\windows\system32\drivers\etc\hosts</span> file for Windows users), which maps domain names to IP addresses. 

Because web servers can host multiple websites from one server when a website is requested from a client, the server knows which website the client wants from the Host header. We can utilize this host header by making changes to it and monitoring the response to see if we've discovered a new website.


```
ffuf -w /usr/share/wordlists/SecLists/Discovery/DNS/namelist.txt -H "Host: FUZZ.acmeitsupport.thm" -u http://10.10.197.103
```

Because the above command will always produce a valid result, we need to filter the output. We can do this by using the page size result with the -fs switch. Edit the below command replacing {size} with the most occurring size value from the previous result

```
-w /usr/share/wordlists/SecLists/Discovery/DNS/namelist.txt -H "Host: FUZZ.acmeitsupport.thm" -u http://10.10.197.103 -fs {size}
```

# Authentication Bypass

## Username Enumeration

A helpful exercise to complete when trying to find authentication vulnerabilities is creating a list of valid usernames, which we'll use later in other tasks.

```
ffuf -w /usr/share/wordlists/SecLists/Usernames/Names/names.txt -X POST -d "username=FUZZ&email=x&password=x&cpassword=x" -H "Content-Type: application/x-www-form-urlencoded" -u http://10.10.16.45/customers/signup -mr "username already exists"
```

<span style="color:rgb(255, 192, 0)">-w</span> argument selects the file's location on the computer that contains the list of usernames
<span style="color:rgb(255, 192, 0)">-X</span> argument specifies the request method, this will be a GET request by default, but it is a POST request in our example
<span style="color:rgb(255, 192, 0)">-d</span> argument specifies the data that we are going to send
<span style="color:rgb(255, 192, 0)">-H</span> argument is used for adding additional headers to the request.<span style="color:rgb(255, 192, 0)">
</span> 
<span style="color:rgb(255, 192, 0)">-u</span> argument specifies the URL we are making the request to
<span style="color:rgb(255, 192, 0)">-mr</span> argument is the text on the page we are looking for to validate we've found a valid username.

**Output will be saved to file named valid_usernames.txt** 

## Brute Force

```
ffuf -w valid_usernames.txt:W1,/usr/share/wordlists/SecLists/Passwords/Common-Credentials/10-million-password-list-top-100.txt:W2 -X POST -d "username=W1&password=W2" -H "Content-Type: application/x-www-form-urlencoded" -u http://10.10.16.45/customers/login -fc 200
```

Because we're using multiple wordlists, we have to specify our own FUZZ keyword. In this instance, we've chosen W1 for our list of valid usernames and W2 for the list of passwords we will try. For a positive match, we're using the <span style="color:rgb(255, 192, 0)">-fc</span> argument to check for an HTTP status code other than 200.

## Logic Flaw

 A logic flaw is when the typical logical path of an application is either bypassed, circumvented or manipulated by a hacker. 

## Cookie Tampering

Examining and editing the cookies set by the web server during your online session can have multiple outcomes, such as unauthenticated access, access to another user's account, or elevated privileges.

### Encoding

 Encoding allows us to convert binary data into human-readable text that can be easily and safely transmitted over mediums that only support plain text ASCII characters.

# IDOR

IDOR (Insecure Direct Object Reference)  can occur when a web server receives user-supplied input to retrieve objects (files, data, documents), too much trust has been placed on the input data, and it is not validated on the server-side to confirm the requested object belongs to the user requesting it.

## Finding IDORs in Encoded IDs

When passing data from page to page either by post data,  web developers will often first take the raw data and encode it. Encoding ensures that the receiving web server will be able to understand the contents. The most common encoding technique on the web is base64 encoding and can usually be pretty easy to spot.

## Hashed IDs

Hashed IDs are a little bit more complicated, but they may follow a predictable pattern, such as being the hashed version of the integer value.

## Finding IDORs in Unpredictable IDs

If the Id cannot be detected using the above methods, an excellent method of IDOR detection is to create two accounts and swap the Id numbers between them. If you can view the other users' content using their Id number while still being logged in with a different account (or not logged in at all), you've found a valid IDOR vulnerability.

## A Practical IDOR Example

 You'll notice the username and email fields pre-filled in with your information. 

We'll start by investigating how this information gets pre-filled. If you open your browser developer tools, select the network tab and then refresh the page, you'll see a call to an endpoint with the path <span style="color:rgb(0, 176, 80)">/api/v1/customer?id={user_id}</span>. This page returns in JSON format your user id, username and email address. We can see from the path that the user information shown is taken from the query string's id parameter 